{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from collections import defaultdict,Counter\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "import re\n",
    "\n",
    "# Load and preprocess Malayalam text from a text file\n",
    "def load_data_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()  # Read lines from the text file\n",
    "    corpus = \"\"\n",
    "    for line in lines:\n",
    "        line = line.strip()  # Remove leading/trailing whitespaces\n",
    "        if line:  # Ignore empty lines\n",
    "            corpus += line + \" \"\n",
    "    \n",
    "    # Preprocessing: Remove non-Malayalam characters and tokenize\n",
    "    corpus = re.sub(r'[^\\u0D00-\\u0D7F\\s]', '', corpus).lower()  # Keep Malayalam chars only\n",
    "    tokens = list(indic_tokenize.trivial_tokenize(corpus, lang='ml'))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Build n-gram model from tokenized text\n",
    "tokens_ml = load_data_from_file('Dataset/train.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create bigrams and trigrams\n",
    "bigrams_ml = list(ngrams(tokens_ml, 2))\n",
    "trigrams_ml = list(ngrams(tokens_ml, 3))\n",
    "unigrams_ml = list(ngrams(tokens_ml, 1))\n",
    "# Frequency counts for bigrams and trigrams\n",
    "unigram_freq_ml = Counter(unigrams_ml)\n",
    "bigram_freq_ml = Counter(bigrams_ml)\n",
    "trigram_freq_ml = Counter(trigrams_ml)\n",
    "word_freq_ml = Counter(tokens_ml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create n-gram models\n",
    "n_grams_freq = defaultdict(Counter)\n",
    "max_n = 3  # Supports up to 5-grams\n",
    "\n",
    "# Build frequency dictionaries for n-grams\n",
    "for n in range(1, max_n + 1):\n",
    "    n_grams = list(ngrams(tokens_ml, n))\n",
    "    n_grams_freq[n].update(n_grams)\n",
    "\n",
    "# Continuation Probability for higher n-grams\n",
    "def continuation_prob_ml(word, n_grams):\n",
    "    unique_preceding = set(w1 for *w1, w2 in n_grams if w2 == word)\n",
    "    return len(unique_preceding) / len(n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kneser-Ney with Interpolation\n",
    "def kneser_ney_interpolated(words, d=0.75):\n",
    "    n = len(words)\n",
    "    if n == 1:\n",
    "        # Unigram case\n",
    "        w1 = words[0]\n",
    "        return continuation_prob_ml(w1, n_grams_freq[2])\n",
    "\n",
    "    # Calculate probabilities\n",
    "    p_kn = 0\n",
    "    for i in range(1, n + 1):\n",
    "        n_gram = tuple(words[-i:])\n",
    "        lower_n_gram = tuple(words[-i + 1:]) if i > 1 else ()\n",
    "        count_ngram = n_grams_freq[i][n_gram]\n",
    "        count_lower = n_grams_freq[i - 1][lower_n_gram] if i > 1 else len(tokens_ml)\n",
    "\n",
    "        # Absolute discounting\n",
    "        if count_lower > 0:\n",
    "            p_ngram = max(count_ngram - d, 0) / count_lower\n",
    "        else:\n",
    "            p_ngram = 0\n",
    "\n",
    "        # Backoff factor\n",
    "        lambda_factor = d * count_lower / (count_lower + len(tokens_ml))\n",
    "        p_continuation = continuation_prob_ml(n_gram[-1], n_grams_freq[i])\n",
    "\n",
    "        # Interpolated probability\n",
    "        p_kn += p_ngram + lambda_factor * p_continuation\n",
    "\n",
    "    return p_kn\n",
    "\n",
    "# Prediction function using higher-order n-grams\n",
    "def predict_next_word_ml(words, top_n=5):\n",
    "    if len(words) < 1:\n",
    "        return {\"error\": \"Provide at least one word for context.\"}\n",
    "\n",
    "    last_n = words[-(max_n - 1):]  # Consider the last few words\n",
    "    candidates = [\n",
    "        (w_next, kneser_ney_interpolated(last_n + [w_next]))\n",
    "        for w_next in unigram_freq_ml\n",
    "    ]\n",
    "    candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "    return candidates[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[171], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mമുമ്പുണ്ടായ ഒരു\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m word_list \u001b[38;5;241m=\u001b[39m words\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpredict_next_word_ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_list\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[166], line 39\u001b[0m, in \u001b[0;36mpredict_next_word_ml\u001b[1;34m(words, top_n)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvide at least one word for context.\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     37\u001b[0m last_n \u001b[38;5;241m=\u001b[39m words[\u001b[38;5;241m-\u001b[39m(max_n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):]  \u001b[38;5;66;03m# Consider the last few words\u001b[39;00m\n\u001b[0;32m     38\u001b[0m candidates \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 39\u001b[0m     (w_next, \u001b[43mkneser_ney_interpolated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_n\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mw_next\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m w_next \u001b[38;5;129;01min\u001b[39;00m unigram_freq_ml\n\u001b[0;32m     41\u001b[0m ]\n\u001b[0;32m     42\u001b[0m candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(candidates, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m candidates[:top_n]\n",
      "Cell \u001b[1;32mIn[166], line 25\u001b[0m, in \u001b[0;36mkneser_ney_interpolated\u001b[1;34m(words, d)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Backoff factor\u001b[39;00m\n\u001b[0;32m     24\u001b[0m lambda_factor \u001b[38;5;241m=\u001b[39m d \u001b[38;5;241m*\u001b[39m count_lower \u001b[38;5;241m/\u001b[39m (count_lower \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens_ml))\n\u001b[1;32m---> 25\u001b[0m p_continuation \u001b[38;5;241m=\u001b[39m \u001b[43mcontinuation_prob_ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_gram\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_grams_freq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Interpolated probability\u001b[39;00m\n\u001b[0;32m     28\u001b[0m p_kn \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p_ngram \u001b[38;5;241m+\u001b[39m lambda_factor \u001b[38;5;241m*\u001b[39m p_continuation\n",
      "Cell \u001b[1;32mIn[160], line 12\u001b[0m, in \u001b[0;36mcontinuation_prob_ml\u001b[1;34m(word, n_grams)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontinuation_prob_ml\u001b[39m(word, n_grams):\n\u001b[1;32m---> 12\u001b[0m     unique_preceding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mw1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mw1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn_grams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_preceding) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(n_grams)\n",
      "Cell \u001b[1;32mIn[160], line 12\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontinuation_prob_ml\u001b[39m(word, n_grams):\n\u001b[1;32m---> 12\u001b[0m     unique_preceding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(w1 \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;241m*\u001b[39mw1, w2 \u001b[38;5;129;01min\u001b[39;00m n_grams \u001b[38;5;28;01mif\u001b[39;00m w2 \u001b[38;5;241m==\u001b[39m word)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_preceding) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(n_grams)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "words = \"മുമ്പുണ്ടായ ഒരു\"\n",
    "word_list = words.split()\n",
    "\n",
    "print(predict_next_word_ml(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuation Probability (count of unique preceding bigram heads)\n",
    "def continuation_prob_ml(word):\n",
    "    unique_preceding = set(w1 for (w1, w2) in bigram_freq_ml if w2 == word)\n",
    "    return len(unique_preceding) / len(bigram_freq_ml)\n",
    "\n",
    "# Kneser-Ney Smoothing for Trigrams\n",
    "def kneser_ney_prob_ml(w1, w2, w3, d=0.75):\n",
    "    trigram_count = trigram_freq_ml[(w1, w2, w3)]\n",
    "    bigram_count = bigram_freq_ml[(w1, w2)]\n",
    "\n",
    "    if bigram_count > 0:\n",
    "        p_trigram = max(trigram_count - d, 0) / bigram_count\n",
    "    else:\n",
    "        p_trigram = 0\n",
    "\n",
    "    lambda_factor = d * bigram_count / (bigram_count + unigram_freq_ml[w2])\n",
    "    p_continuation = continuation_prob_ml(w3)\n",
    "\n",
    "    p_kneser_ney = p_trigram + lambda_factor * p_continuation\n",
    "    return p_kneser_ney\n",
    "\n",
    "# Prediction function using bigrams or trigrams (higher-order n-grams)\n",
    "def predict_next_word_ngram(words, top_n=5):\n",
    "    # print(\"top: \", words)\n",
    "    if len(words) == 1:\n",
    "        # print(\"Bigram model (single word as context)\")\n",
    "        word = words[0]\n",
    "        candidates = [\n",
    "            (w2, bigram_freq_ml[(word, w2)] / word_freq_ml[word]) \n",
    "            for (w1, w2) in bigram_freq_ml if w1 == word\n",
    "        ]\n",
    "        # print(candidates)\n",
    "    elif len(words) >= 2:\n",
    "        # Trigram model (two words as context)\n",
    "        w1, w2 = words[-2], words[-1]\n",
    "        candidates = [\n",
    "        (w3, kneser_ney_prob_ml(w1, w2, w3))\n",
    "        for (_, _, w3) in trigram_freq_ml if (_ == w1 and _ == w2)\n",
    "        ]\n",
    "        print(\"c1: \", candidates)\n",
    "        # word1, word2 = words[-2], words[-1]\n",
    "        # candidates = [\n",
    "        #     (w3, trigram_freq_ml[(word1, word2, w3)] / bigram_freq_ml[(word1, word2)])\n",
    "        #     for (w1, w2, w3) in trigram_freq_ml if w1 == word1 and w2 == word2\n",
    "        # ]\n",
    "        if (len(candidates) == 0) and (isinstance(words,list)) :\n",
    "            # print(type(words))\n",
    "            # print(words)\n",
    "            words.pop(0)\n",
    "            return predict_next_word_ngram(words)\n",
    "    else:\n",
    "        return {\"error\": \"Insufficient words for prediction\"}\n",
    "    \n",
    "    # Sort candidates by probability\n",
    "    # print(\"c1: \", candidates)\n",
    "    candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "    return candidates[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1:  []\n",
      "[('പ്രധാന', 0.015134279637964291), ('വലിയ', 0.010831396211484247), ('ചെറിയ', 0.00964439388693803), ('പ്രത്യേക', 0.009594935456748603), ('ദിവസം', 0.005242593600079133)]\n"
     ]
    }
   ],
   "source": [
    "words = \"മുമ്പുണ്ടായ ഒരു\"\n",
    "word_list = words.split()\n",
    "if len(predict_next_word_ngram(word_list)) == 0:\n",
    "    print(\"no word\")\n",
    "else:\n",
    "    print(predict_next_word_ngram(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from collections import defaultdict, Counter\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "import re\n",
    "\n",
    "# Load and preprocess Malayalam text\n",
    "def load_data_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    corpus = \" \".join([line.strip() for line in lines if line.strip()])\n",
    "    corpus = re.sub(r'[^\\u0D00-\\u0D7F\\s]', '', corpus).lower()\n",
    "    tokens = list(indic_tokenize.trivial_tokenize(corpus, lang='ml'))\n",
    "    return tokens\n",
    "\n",
    "# Load the corpus\n",
    "tokens_ml = load_data_from_file('Dataset/train.txt')\n",
    "\n",
    "# Create n-gram models\n",
    "n_grams_freq = defaultdict(Counter)\n",
    "max_n = 5  # Supports up to 5-grams\n",
    "\n",
    "# Build frequency dictionaries for n-grams\n",
    "for n in range(1, max_n + 1):\n",
    "    n_grams = list(ngrams(tokens_ml, n))\n",
    "    n_grams_freq[n].update(n_grams)\n",
    "\n",
    "# Continuation Probability for higher n-grams\n",
    "def continuation_prob_ml(word, n_grams):\n",
    "    unique_preceding = set(w1 for *w1, w2 in n_grams if w2 == word)\n",
    "    return len(unique_preceding) / len(n_grams)\n",
    "\n",
    "# Kneser-Ney with Interpolation\n",
    "def kneser_ney_interpolated(words, d=0.75):\n",
    "    n = len(words)\n",
    "    if n == 1:\n",
    "        # Unigram case\n",
    "        w1 = words[0]\n",
    "        return continuation_prob_ml(w1, n_grams_freq[2])\n",
    "\n",
    "    # Calculate probabilities\n",
    "    p_kn = 0\n",
    "    for i in range(1, n + 1):\n",
    "        # print(i)\n",
    "        n_gram = tuple(words[-i:])\n",
    "        lower_n_gram = tuple(words[-i + 1:]) if i > 1 else ()\n",
    "        count_ngram = n_grams_freq[i][n_gram]\n",
    "        count_lower = n_grams_freq[i - 1][lower_n_gram] if i > 1 else len(tokens_ml)\n",
    "\n",
    "        # Absolute discounting\n",
    "        if count_lower > 0:\n",
    "            p_ngram = max(count_ngram - d, 0) / count_lower\n",
    "        else:\n",
    "            p_ngram = 0\n",
    "\n",
    "        # Backoff factor\n",
    "        lambda_factor = d * count_lower / (count_lower + len(tokens_ml))\n",
    "        p_continuation = continuation_prob_ml(n_gram[-1], n_grams_freq[i])\n",
    "\n",
    "        # Interpolated probability\n",
    "        p_kn += p_ngram + lambda_factor * p_continuation\n",
    "\n",
    "    return p_kn\n",
    "\n",
    "# Prediction function using higher-order n-grams\n",
    "def predict_next_word_ml(words, top_n=5):\n",
    "    print(\"j\")\n",
    "    if len(words) < 1:\n",
    "        return {\"error\": \"Provide at least one word for context.\"}\n",
    "\n",
    "    last_n = words[-(max_n - 1):]  # Consider the last few words\n",
    "    \n",
    "    candidates = [\n",
    "        (w_next, kneser_ney_interpolated(last_n + [w_next]))\n",
    "        for w_next in n_grams_freq[1]\n",
    "    ]\n",
    "    candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "    print(\"j\")\n",
    "    return candidates[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334519\n"
     ]
    }
   ],
   "source": [
    "print(len(unigram_freq_ml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[178], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mമുമ്പുണ്ടായ ഒരു\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m word_list \u001b[38;5;241m=\u001b[39m words\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mpredict_next_word_ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[177], line 71\u001b[0m, in \u001b[0;36mpredict_next_word_ml\u001b[1;34m(words, top_n)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvide at least one word for context.\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     69\u001b[0m last_n \u001b[38;5;241m=\u001b[39m words[\u001b[38;5;241m-\u001b[39m(max_n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):]  \u001b[38;5;66;03m# Consider the last few words\u001b[39;00m\n\u001b[0;32m     70\u001b[0m candidates \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 71\u001b[0m     (w_next, \u001b[43mkneser_ney_interpolated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_n\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mw_next\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m w_next \u001b[38;5;129;01min\u001b[39;00m unigram_freq_ml\n\u001b[0;32m     73\u001b[0m ]\n\u001b[0;32m     74\u001b[0m candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(candidates, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m candidates[:top_n]\n",
      "Cell \u001b[1;32mIn[177], line 57\u001b[0m, in \u001b[0;36mkneser_ney_interpolated\u001b[1;34m(words, d)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Backoff factor\u001b[39;00m\n\u001b[0;32m     56\u001b[0m lambda_factor \u001b[38;5;241m=\u001b[39m d \u001b[38;5;241m*\u001b[39m count_lower \u001b[38;5;241m/\u001b[39m (count_lower \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens_ml))\n\u001b[1;32m---> 57\u001b[0m p_continuation \u001b[38;5;241m=\u001b[39m \u001b[43mcontinuation_prob_ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_gram\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_grams_freq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Interpolated probability\u001b[39;00m\n\u001b[0;32m     60\u001b[0m p_kn \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p_ngram \u001b[38;5;241m+\u001b[39m lambda_factor \u001b[38;5;241m*\u001b[39m p_continuation\n",
      "Cell \u001b[1;32mIn[177], line 29\u001b[0m, in \u001b[0;36mcontinuation_prob_ml\u001b[1;34m(word, n_grams)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontinuation_prob_ml\u001b[39m(word, n_grams):\n\u001b[1;32m---> 29\u001b[0m     unique_preceding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mw1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mw1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn_grams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_preceding) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(n_grams)\n",
      "Cell \u001b[1;32mIn[177], line 29\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontinuation_prob_ml\u001b[39m(word, n_grams):\n\u001b[1;32m---> 29\u001b[0m     unique_preceding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(w1 \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;241m*\u001b[39mw1, w2 \u001b[38;5;129;01min\u001b[39;00m n_grams \u001b[38;5;28;01mif\u001b[39;00m w2 \u001b[38;5;241m==\u001b[39m word)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_preceding) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(n_grams)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "words = \"മുമ്പുണ്ടായ ഒരു\"\n",
    "word_list = words.split()\n",
    "predict_next_word_ml(word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['അവൻ', 'സ്കൂളിലേക്ക്', 'പോയി']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "import re\n",
    "\n",
    "# Example Malayalam text\n",
    "text_ml = \"അവൻ സ്കൂളിലേക്ക് പോയി.\"\n",
    "\n",
    "# Preprocess: Lowercasing and punctuation removal\n",
    "text_ml = re.sub(r'[^\\u0D00-\\u0D7F\\s]', '', text_ml)  # Keep Malayalam chars only\n",
    "\n",
    "# Tokenize\n",
    "tokens_ml = list(indic_tokenize.trivial_tokenize(text_ml, lang='ml'))\n",
    "print(tokens_ml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_word_ml(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
